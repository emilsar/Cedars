{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e509b29b-7ec3-47a2-a9ea-82e1b8057591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_geometric'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Linear\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Data, InMemoryDataset\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (GCNConv, GATConv, DeepGraphInfomax, SAGEConv,\n\u001b[1;32m     25\u001b[0m                                 DenseGraphConv, GINEConv, APPNP, LEConv)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (global_mean_pool, TopKPooling, SAGPooling,\n\u001b[1;32m     27\u001b[0m                                 GlobalAttention, BatchNorm, JumpingKnowledge)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import argparse\n",
    "import cv2\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import umap\n",
    "import numba\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.data import DataLoader, Data, InMemoryDataset\n",
    "from torch_geometric.nn import (GCNConv, GATConv, DeepGraphInfomax, SAGEConv,\n",
    "                                DenseGraphConv, GINEConv, APPNP, LEConv)\n",
    "from torch_geometric.nn import (global_mean_pool, TopKPooling, SAGPooling,\n",
    "                                GlobalAttention, BatchNorm, JumpingKnowledge)\n",
    "from torch_geometric.utils import (dropout_adj, train_test_split_edges,\n",
    "                                   add_remaining_self_loops, softmax,\n",
    "                                   to_networkx, to_dense_batch, to_dense_adj,\n",
    "                                   dense_to_sparse)\n",
    "from torch_cluster import knn_graph\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_scatter import scatter\n",
    "\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "class NodeNorm(nn.Module): \n",
    "    # https://github.com/miafei/NodeNorm/blob/master/layers.py\n",
    "    def __init__(self, unbiased=False, eps=1e-5):\n",
    "        super(NodeNorm, self).__init__()\n",
    "        self.unbiased = unbiased\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        std = (torch.var(x, unbiased=self.unbiased, dim=1, keepdim=True) + self.eps).sqrt()\n",
    "        x = (x - mean) / std\n",
    "        return x\n",
    "\n",
    "\n",
    "class GCNNet(torch.nn.Module): \n",
    "    def __init__(self, inp_dim, out_dim, hidden_topology=[32,64,128,128], p=0.5, p2=0.1, drop_each=True, attn_dropout=False, n_heads=1):\n",
    "        super(GCNNet, self).__init__()\n",
    "        self.out_dim=out_dim\n",
    "        self.hidden_topology=hidden_topology\n",
    "        self.norms=[]\n",
    "        self.convs = nn.ModuleList([GATConv(inp_dim, hidden_topology[0], heads=n_heads, dropout=p*float(attn_dropout))]+[GATConv(n_heads*hidden_topology[i],hidden_topology[i+1], heads=n_heads, dropout=p*float(attn_dropout)) for i in range(len(hidden_topology[:-1]))])\n",
    "        for i,conv in enumerate(self.convs):\n",
    "            conv.reset_parameters()\n",
    "            self.norms.append(BatchNorm(hidden_topology[i]*n_heads, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
    "        self.norms = nn.ModuleList(self.norms)\n",
    "        self.drop_edge = lambda edge_index: dropout_adj(edge_index,p=p2)[0]\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(hidden_topology[-1]*n_heads, out_dim)\n",
    "        self.drop_each=drop_each\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        for i,conv in enumerate(self.convs):\n",
    "            if self.drop_each and self.training: edge_index=self.drop_edge(edge_index)\n",
    "            x = self.norms[i](F.relu(conv(x, edge_index, edge_attr)))\n",
    "        if self.training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x,edge_index\n",
    "    \n",
    "class GCNFeatures(torch.nn.Module):\n",
    "    def __init__(self, gcn, bayes=False):\n",
    "        super(GCNFeatures, self).__init__()\n",
    "        self.gcn=gcn\n",
    "        self.drop_each=bayes\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        for i,conv in enumerate(self.gcn.convs):\n",
    "            if self.drop_each: edge_index=self.gcn.drop_edge(edge_index)\n",
    "            x = self.gcn.norms[i](F.relu(conv(x, edge_index, edge_attr)))\n",
    "        if self.drop_each:\n",
    "            x = self.gcn.dropout(x)\n",
    "        y = self.gcn.fc(x)\n",
    "        return x,y,edge_index\n",
    "    \n",
    "\n",
    "class WSG(torch.nn.Module):\n",
    "    def __init__(self, inp_dim, n_hidden=32, n_layers=3, n_conv_block=3, p=0.5, p2=0.1, drop_each=True,n_out=2,dropout=0.4,ratio=0.5,min_score=None,attn_dropout=True,n_heads=1):# ratio 0.5\n",
    "        super(WSG, self).__init__()\n",
    "        if n_heads>1: n_heads=1\n",
    "        self.conv1=GCNNet(inp_dim,n_hidden*n_heads,[n_hidden]*n_layers,p=p, p2=p2, drop_each=True, attn_dropout=True,n_heads=n_heads)\n",
    "        self.convs=nn.ModuleList([GCNNet(n_hidden*n_heads, n_hidden*n_heads, hidden_topology=[n_hidden]*n_layers, p=p, p2=p2, drop_each=True, attn_dropout=True, n_heads=n_heads) for i in range(n_conv_block)])\n",
    "        self.pools=nn.ModuleList([SAGPooling(n_hidden*n_heads,dropout=dropout,ratio=ratio,min_score=min_score) for i in range(n_conv_block)])\n",
    "        self.mean_pool=lambda x,b: global_mean_pool(x,b)\n",
    "        self.n_out=n_out\n",
    "        self.fc=nn.Linear(n_hidden,n_out)#*(len(self.convs)+1)*n_heads\n",
    "        self.attention = GlobalAttention(gate_nn=nn.Sequential(nn.Linear(n_hidden * n_heads, 1),\n",
    "                                                                nn.Sigmoid()))\n",
    "        \n",
    "    def forward(self, x, edge_index, batch, edge_attr=None):\n",
    "        x,edge_index=self.conv1(x,edge_index)\n",
    "        xs=[self.mean_pool(x,batch)]\n",
    "        perms=[]\n",
    "        for conv,pool in zip(self.convs,self.pools):\n",
    "            x,edge_index=conv(x,edge_index)\n",
    "            x,edge_index, _, batch, perm, _=pool(x,edge_index,batch=batch)\n",
    "        z=self.attention(x,batch)\n",
    "        return z,self.fc((F.relu(z) if not self.training else self.conv1.dropout(F.relu(z)))),perms\n",
    "\n",
    "class SHAP_GNN_GraphLevel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, \n",
    "                 train_size=0.8, \n",
    "                 objective='class',\n",
    "                use_weights=True,\n",
    "                n_batches_backward=1,\n",
    "                f1_metric='weighted',\n",
    "                n_epochs=20,\n",
    "                out_dir='gnn_models',\n",
    "                lr=1e-3,\n",
    "                eta_min=1e-4,\n",
    "                T_max=20,\n",
    "                wd=0,\n",
    "                hidden_topology=[32,64,128,128],\n",
    "                p=0.3,\n",
    "                p2=0.3,\n",
    "                burnin=400,\n",
    "                warmup=100,\n",
    "                gpu_id=0,\n",
    "                n_conv_block=1,\n",
    "                ratio=0.8,\n",
    "                min_score=None,\n",
    "                batch_size=128,\n",
    "                verbose=False):\n",
    "        assert objective=='class'\n",
    "        self.train_size=train_size \n",
    "        self.objective=objective\n",
    "        self.use_weights=use_weights\n",
    "        self.n_batches_backward=n_batches_backward\n",
    "        self.f1_metric=f1_metric\n",
    "        self.n_epochs=n_epochs\n",
    "        self.out_dir=out_dir\n",
    "        self.lr=lr\n",
    "        self.eta_min=eta_min\n",
    "        self.T_max=T_max\n",
    "        self.wd=wd\n",
    "        self.hidden_topology=hidden_topology\n",
    "        self.p=p\n",
    "        self.p2=p2\n",
    "        self.burnin=burnin\n",
    "        self.warmup=warmup\n",
    "        self.gpu_id=gpu_id\n",
    "        self.n_conv_block=n_conv_block\n",
    "        self.ratio=ratio\n",
    "        self.min_score=min_score\n",
    "        self.batch_size=batch_size\n",
    "        self.verbose=verbose\n",
    "        \n",
    "    def fit(self, X,y,sample_weight=None, group_membership=None):\n",
    "        assert sample_weight is None\n",
    "        assert group_membership is None\n",
    "        train_data,val_data,y_train,y_val=train_test_split(X,y,train_size=0.5,stratify=y,random_state=42)\n",
    "            \n",
    "        if self.use_weights: \n",
    "            weights=compute_class_weight(\n",
    "                class_weight = \"balanced\",\n",
    "                classes = np.unique(y),\n",
    "                y = y_train)\n",
    "        else: \n",
    "            weights=None\n",
    "\n",
    "        self.model=WSG(train_data[0].x.shape[1],  \n",
    "                        n_hidden=self.hidden_topology[0], \n",
    "                        n_layers=len(self.hidden_topology), \n",
    "                        n_conv_block=self.n_conv_block, \n",
    "                        p=self.p, \n",
    "                        p2=self.p2, \n",
    "                        drop_each=True,\n",
    "                        n_out=len(np.unique(y)),\n",
    "                        dropout=self.p,\n",
    "                        ratio=self.ratio,\n",
    "                        min_score=self.min_score)\n",
    "        if torch.cuda.is_available(): self.model=self.model.cuda()\n",
    "\n",
    "        # load optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=self.lr,weight_decay=self.wd)\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, self.T_max, eta_min=self.eta_min, last_epoch=-1)\n",
    "        if self.use_weights:\n",
    "            weights=torch.tensor(weights).float()\n",
    "            if torch.cuda.is_available(): weights=weights.cuda()\n",
    "        else: weights=None\n",
    "        self.criterion=nn.CrossEntropyLoss(weight=weights)\n",
    "        if torch.cuda.is_available(): self.criterion=self.criterion.cuda()\n",
    "\n",
    "        # initialize val saving\n",
    "        save_mod=True\n",
    "        past_performance=[0]\n",
    "        best_model_dict=copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "        # dataloaders\n",
    "        dataloaders={}\n",
    "\n",
    "        dataloaders['train']=DataLoader(train_data,batch_size=self.batch_size,shuffle=True)\n",
    "        dataloaders['val']=DataLoader(val_data,batch_size=self.batch_size,shuffle=True)\n",
    "        train_loader=dataloaders['train']\n",
    "\n",
    "        n_total_batches=0\n",
    "        train_val_auc=[]\n",
    "        for epoch in tqdm.trange(self.n_epochs):\n",
    "            Y,Y_Pred=[],[]\n",
    "            Y2,Y_Pred2,Y_Pred3=[],[],[]\n",
    "            self.model.train(True)\n",
    "            for i,data in enumerate(train_loader):\n",
    "                n_total_batches+=1\n",
    "                if torch.cuda.is_available():\n",
    "                    x=data.x.cuda()\n",
    "                    edge_index=data.edge_index.cuda()\n",
    "                    y=data.y.cuda()\n",
    "                    batch=data.batch.cuda()\n",
    "                y=y.long()\n",
    "                y_pred=self.model(x,edge_index,batch)[1]\n",
    "                loss = self.criterion(y_pred, y) / self.n_batches_backward\n",
    "                loss.backward()\n",
    "                if n_total_batches%self.n_batches_backward==0 or (i==len(train_loader.dataset)-1):\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                Y_Pred.append(F.softmax(y_pred).detach().cpu().numpy())\n",
    "                Y.append(y.detach().cpu().numpy().flatten())\n",
    "                del x, edge_index, loss\n",
    "            Y,Y_Pred=np.hstack(Y),np.vstack(Y_Pred)\n",
    "            train_auc=roc_auc_score(Y,Y_Pred if Y_Pred.shape[1]>2 else Y_Pred[:,1],average='macro',multi_class='ovr')\n",
    "            self.scheduler.step()\n",
    "            self.model.train(False)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for k in ['val']:\n",
    "                    Y,Y_Pred,Z=[],[],[]\n",
    "                    for i,data in enumerate(dataloaders[k]):\n",
    "                        if torch.cuda.is_available():\n",
    "                            x=data.x.cuda()\n",
    "                            edge_index=data.edge_index.cuda()\n",
    "                            y=data.y.cuda()\n",
    "                            batch=data.batch.cuda()\n",
    "                        y=y.long()\n",
    "                        z_out,y_pred,perm=self.model(x,edge_index,batch)\n",
    "                        y_pred_proba=F.softmax(y_pred).detach().cpu().numpy()\n",
    "                        y_true=data.y.numpy().flatten()\n",
    "                        Y.append(y_true)\n",
    "                        Y_Pred.append(y_pred_proba)\n",
    "                        Z.append(z_out.detach().cpu().numpy())\n",
    "                        del x, edge_index, z_out, y_pred_proba\n",
    "                    Y_true,Y_Pred=np.hstack(Y),np.vstack(Y_Pred)\n",
    "                   \n",
    "                    val_auc=roc_auc_score(Y_true,Y_Pred if Y_Pred.shape[1]>2 else Y_Pred[:,1],average='macro',multi_class='ovr')\n",
    "                    if save_mod and val_auc>=max(past_performance):\n",
    "                        best_model_dict=copy.deepcopy(self.model.state_dict())\n",
    "                    past_performance.append(val_auc)\n",
    "            if self.verbose: print(train_auc,val_auc)\n",
    "        \n",
    "        print(f\"Top AUC: {max(past_performance)}\")\n",
    "        self.model.load_state_dict(best_model_dict)\n",
    "        self.past_performance=past_performance\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self,X):\n",
    "        dataloaders={}\n",
    "        dataloaders['test']=DataLoader(X,batch_size=128,shuffle=False)\n",
    "        with torch.no_grad():\n",
    "            for k in ['test']:\n",
    "                Y,Y_Pred,Z=[],[],[]\n",
    "                for i,data in enumerate(dataloaders[k]):\n",
    "                    if torch.cuda.is_available():\n",
    "                        x=data.x.cuda()\n",
    "                        edge_index=data.edge_index.cuda()\n",
    "                        y=data.y.cuda()\n",
    "                        batch=data.batch.cuda()\n",
    "                    z_out,y_pred,perm=self.model(x,edge_index,batch)\n",
    "                    y_pred_proba=F.softmax(y_pred).detach().cpu().numpy()\n",
    "                    y_true=data.y.numpy().flatten()\n",
    "                    Y.append(y_true)\n",
    "                    Y_Pred.append(y_pred_proba)\n",
    "                    Z.append(z_out.detach().cpu().numpy())\n",
    "                    del x, edge_index, z_out, y_pred_proba\n",
    "                Y_true=np.hstack(Y)\n",
    "                return np.vstack(Y_Pred)\n",
    "\n",
    "    def predict(self,X):\n",
    "        return self.predict_proba(X).argmax(1)\n",
    "        \n",
    "        \n",
    "class SHAP_GNN_NodeLevel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, \n",
    "                 train_size=0.8, \n",
    "                 objective='class',\n",
    "                use_weights=True,\n",
    "                n_batches_backward=1,\n",
    "                f1_metric='weighted',\n",
    "                n_epochs=20,\n",
    "                out_dir='gnn_models',\n",
    "                lr=1e-3,\n",
    "                eta_min=1e-4,\n",
    "                T_max=20,\n",
    "                wd=0,\n",
    "                hidden_topology=[32,64,128,128],\n",
    "                p=0.3,\n",
    "                p2=0.3,\n",
    "                burnin=400,\n",
    "                warmup=100,\n",
    "                gpu_id=0,\n",
    "                n_conv_block=1,\n",
    "                ratio=0.8,\n",
    "                min_score=None,\n",
    "                batch_size=128,\n",
    "                verbose=False):\n",
    "        assert objective=='class'\n",
    "        self.train_size=train_size \n",
    "        self.objective=objective\n",
    "        self.use_weights=use_weights\n",
    "        self.n_batches_backward=n_batches_backward\n",
    "        self.f1_metric=f1_metric\n",
    "        self.n_epochs=n_epochs\n",
    "        self.out_dir=out_dir\n",
    "        self.lr=lr\n",
    "        self.eta_min=eta_min\n",
    "        self.T_max=T_max\n",
    "        self.wd=wd\n",
    "        self.hidden_topology=hidden_topology\n",
    "        self.p=p\n",
    "        self.p2=p2\n",
    "        self.burnin=burnin\n",
    "        self.warmup=warmup\n",
    "        self.gpu_id=gpu_id\n",
    "        self.n_conv_block=n_conv_block\n",
    "        self.ratio=ratio\n",
    "        self.min_score=min_score\n",
    "        self.batch_size=batch_size\n",
    "        self.verbose=verbose\n",
    "        \n",
    "    def fit(self, X,y,sample_weight=None, group_membership=None):\n",
    "        assert sample_weight is None\n",
    "        assert group_membership is None\n",
    "        train_data,val_data,y_train,y_val=train_test_split(X,y,train_size=0.8,random_state=42)\n",
    "        y=list(reduce(lambda x,y:(x)+(y),y))\n",
    "            \n",
    "        y_train=list(reduce(lambda x,y:(x)+(y),y_train))\n",
    "\n",
    "        if self.use_weights: \n",
    "            weights=compute_class_weight(\n",
    "                class_weight = \"balanced\",\n",
    "                classes = np.unique(y_train),\n",
    "                y = y_train)\n",
    "        else: \n",
    "            weights=None\n",
    "\n",
    "        self.model=GCNNet(train_data[0].x.shape[1], len(np.unique(y)), \n",
    "                          hidden_topology=self.hidden_topology, \n",
    "                          p=self.p, \n",
    "                          p2=self.p2)\n",
    "        if torch.cuda.is_available(): self.model=self.model.cuda()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=self.lr,weight_decay=self.wd)\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, self.T_max, eta_min=self.eta_min, last_epoch=-1)\n",
    "        if self.use_weights:\n",
    "            weights=torch.tensor(weights).float()\n",
    "            if torch.cuda.is_available(): weights=weights.cuda()\n",
    "        else: weights=None\n",
    "        self.criterion=nn.CrossEntropyLoss(weight=weights)\n",
    "        if torch.cuda.is_available(): self.criterion=self.criterion.cuda()\n",
    "\n",
    "        save_mod=True\n",
    "        past_performance=[0]\n",
    "        best_model_dict=copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "        dataloaders={}\n",
    "\n",
    "        dataloaders['train']=DataLoader(train_data,batch_size=self.batch_size,shuffle=True)\n",
    "        dataloaders['val']=DataLoader(val_data,batch_size=self.batch_size,shuffle=True)\n",
    "        train_loader=dataloaders['train']\n",
    "\n",
    "        n_total_batches=0\n",
    "        train_val_auc=[]\n",
    "        for epoch in tqdm.trange(self.n_epochs):\n",
    "            Y,Y_Pred=[],[]\n",
    "            Y2,Y_Pred2,Y_Pred3=[],[],[]\n",
    "            self.model.train(True)\n",
    "            for i,data in enumerate(train_loader):\n",
    "                n_total_batches+=1\n",
    "                if torch.cuda.is_available():\n",
    "                    x=data.x.cuda()\n",
    "                    edge_index=data.edge_index.cuda()\n",
    "                    y=data.y.cuda()\n",
    "                    batch=data.batch.cuda()\n",
    "                y=y.long()\n",
    "                y_pred=self.model(x,edge_index)[0]\n",
    "                y[-1] = 1\n",
    "                loss = self.criterion(y_pred, y) / self.n_batches_backward\n",
    "                loss.backward()\n",
    "                if n_total_batches%self.n_batches_backward==0 or (i==len(train_loader.dataset)-1):\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                Y_Pred.append(F.softmax(y_pred,dim=1).detach().cpu().numpy())\n",
    "                Y.append(y.detach().cpu().numpy().flatten())\n",
    "                del x, edge_index, loss\n",
    "            Y_true,Y_Pred=np.hstack(Y),np.vstack(Y_Pred)\n",
    "            train_auc=roc_auc_score(Y_true,Y_Pred if Y_Pred.shape[1]>2 else Y_Pred[:,1],average='macro',multi_class='ovr')\n",
    "            self.scheduler.step()\n",
    "            self.model.train(False)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for k in ['val']:\n",
    "                    Y,Y_Pred,Z=[],[],[]\n",
    "                    for i,data in enumerate(dataloaders[k]):\n",
    "                        if torch.cuda.is_available():\n",
    "                            x=data.x.cuda()\n",
    "                            edge_index=data.edge_index.cuda()\n",
    "                            y=data.y.cuda()\n",
    "                            batch=data.batch.cuda()\n",
    "                        y=y.long()\n",
    "                        y_pred=self.model(x,edge_index)[0]\n",
    "                        y_pred_proba=F.softmax(y_pred,dim=1).detach().cpu().numpy()\n",
    "                        y_true=data.y.numpy().flatten()\n",
    "                        Y.append(y_true)\n",
    "                        Y_Pred.append(y_pred_proba)\n",
    "                        del x, edge_index, y_pred_proba\n",
    "                    Y_true,Y_Pred=np.hstack(Y),np.vstack(Y_Pred)\n",
    "                    val_auc=roc_auc_score(Y_true,Y_Pred if Y_Pred.shape[1]>2 else Y_Pred[:,1],average='weighted',multi_class='ovr')\n",
    "\n",
    "                    if save_mod and val_auc>=max(past_performance):\n",
    "                        best_model_dict=copy.deepcopy(self.model.state_dict())\n",
    "                    past_performance.append(val_auc)\n",
    "\n",
    "            if self.verbose: print(train_auc,val_auc)\n",
    "        self.past_performance=past_performance\n",
    "        self.model.load_state_dict(best_model_dict)\n",
    "        print(f\"Top AUC: {max(past_performance)}\")\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self,X):\n",
    "        dataloaders={}\n",
    "        dataloaders['test']=DataLoader(X,batch_size=128,shuffle=False)\n",
    "        with torch.no_grad():\n",
    "            for k in ['test']:\n",
    "                Y,Y_Pred,Z=[],[],[]\n",
    "                for i,data in enumerate(dataloaders[k]):\n",
    "                    if torch.cuda.is_available():\n",
    "                        x=data.x.cuda()\n",
    "                        edge_index=data.edge_index.cuda()\n",
    "                        y=data.y.cuda()\n",
    "                        batch=data.batch.cuda()\n",
    "                    y_pred=self.model(x,edge_index)[0]\n",
    "                    y_pred_proba=F.softmax(y_pred,dim=1).detach().cpu().numpy()\n",
    "                    y_true=data.y.numpy().flatten()\n",
    "                    Y.append(y_true)\n",
    "                    Y_Pred.append(y_pred_proba)\n",
    "                    del x, edge_index, y_pred_proba\n",
    "                Y_true=np.hstack(Y)\n",
    "                return np.vstack(Y_Pred)\n",
    "\n",
    "    def predict(self,X):\n",
    "        return self.predict_proba(X).argmax(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9233e0a9-4d78-4218-b4ad-7d763d9a944c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch; torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd8b9ca-3e53-41ef-90fa-2aea7f9ba774",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.5.0-py3-none-any.whl.metadata (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m592.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from torch_geometric) (4.66.1)\n",
      "Requirement already satisfied: numpy in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from torch_geometric) (1.23.5)\n",
      "Requirement already satisfied: scipy in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from torch_geometric) (1.11.2)\n",
      "Requirement already satisfied: fsspec in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from torch_geometric) (2023.1.0)\n",
      "Requirement already satisfied: jinja2 in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: aiohttp in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from torch_geometric) (3.8.5)\n",
      "Requirement already satisfied: requests in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from torch_geometric) (2.28.2)\n",
      "Requirement already satisfied: pyparsing in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from torch_geometric) (1.4.1.post1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from torch_geometric) (5.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from aiohttp->torch_geometric) (23.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from aiohttp->torch_geometric) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from aiohttp->torch_geometric) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from aiohttp->torch_geometric) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from jinja2->torch_geometric) (2.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from requests->torch_geometric) (3.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from requests->torch_geometric) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from requests->torch_geometric) (2024.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from scikit-learn->torch_geometric) (3.3.0)\n",
      "Downloading torch_geometric-2.5.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.5.0\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cpu.html\n",
      "Collecting torch_scatter\n",
      "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_scatter-2.1.2-cp311-cp311-macosx_10_9_universal2.whl (638 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m638.3/638.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch_sparse\n",
      "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_sparse-0.6.18-cp311-cp311-macosx_10_9_universal2.whl (970 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m970.3/970.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch_cluster\n",
      "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_cluster-1.6.3-cp311-cp311-macosx_10_9_universal2.whl (571 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.6/571.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch_spline_conv\n",
      "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcpu/torch_spline_conv-1.2.2-cp311-cp311-macosx_10_9_universal2.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.8/200.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from torch_sparse) (1.11.2)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /Users/joshualevy/opt/anaconda3/envs/py311/lib/python3.11/site-packages (from scipy->torch_sparse) (1.23.5)\n",
      "Installing collected packages: torch_spline_conv, torch_scatter, torch_sparse, torch_cluster\n",
      "Successfully installed torch_cluster-1.6.3 torch_scatter-2.1.2 torch_sparse-0.6.18 torch_spline_conv-1.2.2\n"
     ]
    }
   ],
   "source": [
    "! pip install torch_geometric\n",
    "\n",
    "! pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.1+cpu.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292cde8b-4879-477c-8111-011776638822",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgnn=SHAP_GNN_NodeLevel(lr=1e-2,n_epochs=500,burnin=0,warmup=0,eta_min=1e-6, hidden_topology=[32,32], T_max = 60, n_batches_backward=1).fit(X_raw,y_raw)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
